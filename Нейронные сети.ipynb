{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Ход работы"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Импортируем библиотеки\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Считываем набор данных\n",
        "train_data = pd.read_csv('\/data\/notebook_files\/train.csv')\n",
        "test_data = pd.read_csv('\/data\/notebook_files\/test.csv')"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Подготовка данных"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Смотрим, как выглядят тренировочные и тестовые данные"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Выводим 5 строк тренировочных данных\n",
        "train_data.head()"
      ],
      "execution_count":2,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>Id<\/th>\n",
              "      <th>MSSubClass<\/th>\n",
              "      <th>MSZoning<\/th>\n",
              "      <th>LotFrontage<\/th>\n",
              "      <th>LotArea<\/th>\n",
              "      <th>Street<\/th>\n",
              "      <th>Alley<\/th>\n",
              "      <th>LotShape<\/th>\n",
              "      <th>LandContour<\/th>\n",
              "      <th>Utilities<\/th>\n",
              "      <th>...<\/th>\n",
              "      <th>PoolArea<\/th>\n",
              "      <th>PoolQC<\/th>\n",
              "      <th>Fence<\/th>\n",
              "      <th>MiscFeature<\/th>\n",
              "      <th>MiscVal<\/th>\n",
              "      <th>MoSold<\/th>\n",
              "      <th>YrSold<\/th>\n",
              "      <th>SaleType<\/th>\n",
              "      <th>SaleCondition<\/th>\n",
              "      <th>SalePrice<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>1<\/td>\n",
              "      <td>60<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>65.0<\/td>\n",
              "      <td>8450<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>Reg<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>2<\/td>\n",
              "      <td>2008<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "      <td>208500<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>2<\/td>\n",
              "      <td>20<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>80.0<\/td>\n",
              "      <td>9600<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>Reg<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>5<\/td>\n",
              "      <td>2007<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "      <td>181500<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>2<\/th>\n",
              "      <td>3<\/td>\n",
              "      <td>60<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>68.0<\/td>\n",
              "      <td>11250<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>9<\/td>\n",
              "      <td>2008<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "      <td>223500<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>3<\/th>\n",
              "      <td>4<\/td>\n",
              "      <td>70<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>60.0<\/td>\n",
              "      <td>9550<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>2<\/td>\n",
              "      <td>2006<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Abnorml<\/td>\n",
              "      <td>140000<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>4<\/th>\n",
              "      <td>5<\/td>\n",
              "      <td>60<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>84.0<\/td>\n",
              "      <td>14260<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>12<\/td>\n",
              "      <td>2008<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "      <td>250000<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<p>5 rows × 81 columns<\/p>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Выводим 5 строк тестовых данных\n",
        "test_data.head()"
      ],
      "execution_count":3,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>Id<\/th>\n",
              "      <th>MSSubClass<\/th>\n",
              "      <th>MSZoning<\/th>\n",
              "      <th>LotFrontage<\/th>\n",
              "      <th>LotArea<\/th>\n",
              "      <th>Street<\/th>\n",
              "      <th>Alley<\/th>\n",
              "      <th>LotShape<\/th>\n",
              "      <th>LandContour<\/th>\n",
              "      <th>Utilities<\/th>\n",
              "      <th>...<\/th>\n",
              "      <th>ScreenPorch<\/th>\n",
              "      <th>PoolArea<\/th>\n",
              "      <th>PoolQC<\/th>\n",
              "      <th>Fence<\/th>\n",
              "      <th>MiscFeature<\/th>\n",
              "      <th>MiscVal<\/th>\n",
              "      <th>MoSold<\/th>\n",
              "      <th>YrSold<\/th>\n",
              "      <th>SaleType<\/th>\n",
              "      <th>SaleCondition<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>1461<\/td>\n",
              "      <td>20<\/td>\n",
              "      <td>RH<\/td>\n",
              "      <td>80.0<\/td>\n",
              "      <td>11622<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>Reg<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>120<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>MnPrv<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>6<\/td>\n",
              "      <td>2010<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>1462<\/td>\n",
              "      <td>20<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>81.0<\/td>\n",
              "      <td>14267<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>Gar2<\/td>\n",
              "      <td>12500<\/td>\n",
              "      <td>6<\/td>\n",
              "      <td>2010<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>2<\/th>\n",
              "      <td>1463<\/td>\n",
              "      <td>60<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>74.0<\/td>\n",
              "      <td>13830<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>MnPrv<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>3<\/td>\n",
              "      <td>2010<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>3<\/th>\n",
              "      <td>1464<\/td>\n",
              "      <td>60<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>78.0<\/td>\n",
              "      <td>9978<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>Lvl<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>6<\/td>\n",
              "      <td>2010<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>4<\/th>\n",
              "      <td>1465<\/td>\n",
              "      <td>120<\/td>\n",
              "      <td>RL<\/td>\n",
              "      <td>43.0<\/td>\n",
              "      <td>5005<\/td>\n",
              "      <td>Pave<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>IR1<\/td>\n",
              "      <td>HLS<\/td>\n",
              "      <td>AllPub<\/td>\n",
              "      <td>...<\/td>\n",
              "      <td>144<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>0<\/td>\n",
              "      <td>1<\/td>\n",
              "      <td>2010<\/td>\n",
              "      <td>WD<\/td>\n",
              "      <td>Normal<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<p>5 rows × 80 columns<\/p>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# функция для проверки и подсчёта пропущеных значений в test_data\n",
        "# Выводит  датафрейм: Название метркик, кол-во пропущенных значений, тип данных этих значений.\n",
        "# Отсортировано в порядке убывания для наглядности\n",
        "def missing_value_checker(data):\n",
        "    columns = ['name','empty_sum','type']\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "    for feature, content in data.items():\n",
        "        if data[feature].isnull().values.any():\n",
        "            empty_sum = data[feature].isna().sum()\n",
        "            data_type = data[feature].dtype\n",
        "            new_row = {'name': feature, 'empty_sum': empty_sum, 'type':data_type}\n",
        "            df = df.append(new_row, ignore_index=True)\n",
        "    sorted_df = df.sort_values(by='empty_sum',ascending=False)\n",
        "    print(sorted_df)\n",
        "    print(f'Кол-во метрик с пустыми значениями {df.shape[0]}')\n",
        "\n",
        "missing_value_checker(test_data)"
      ],
      "execution_count":4,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "            name empty_sum     type\n",
            "29        PoolQC      1456   object\n",
            "31   MiscFeature      1408   object\n",
            "2          Alley      1352   object\n",
            "30         Fence      1169   object\n",
            "21   FireplaceQu       730   object\n",
            "1    LotFrontage       227  float64\n",
            "28    GarageCond        78   object\n",
            "23   GarageYrBlt        78  float64\n",
            "27    GarageQual        78   object\n",
            "24  GarageFinish        78   object\n",
            "22    GarageType        76   object\n",
            "9       BsmtCond        45   object\n",
            "10  BsmtExposure        44   object\n",
            "8       BsmtQual        44   object\n",
            "11  BsmtFinType1        42   object\n",
            "13  BsmtFinType2        42   object\n",
            "6     MasVnrType        16   object\n",
            "7     MasVnrArea        15  float64\n",
            "0       MSZoning         4   object\n",
            "17  BsmtFullBath         2  float64\n",
            "18  BsmtHalfBath         2  float64\n",
            "20    Functional         2   object\n",
            "3      Utilities         2   object\n",
            "25    GarageCars         1  float64\n",
            "26    GarageArea         1  float64\n",
            "16   TotalBsmtSF         1  float64\n",
            "19   KitchenQual         1   object\n",
            "15     BsmtUnfSF         1  float64\n",
            "14    BsmtFinSF2         1  float64\n",
            "12    BsmtFinSF1         1  float64\n",
            "5    Exterior2nd         1   object\n",
            "4    Exterior1st         1   object\n",
            "32      SaleType         1   object\n",
            "Кол-во метрик с пустыми значениями 33\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Проверяем какие признаки в таблице можно оставить, а какие удалить. Если пропущенных значений слишком много, то удалим признак. Если их небольшое количество, то заполним mean или median для чисел, новая категория missing для строковых объектов.\n",
        "\n",
        "В соответствии с этим:<br>\n",
        "– удалим ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'];<br>\n",
        "– заполним числовое отсутствующее значение значением медианы;<br>\n",
        "– заполним строковое отсутствующее значение значением missing."
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Удаляем строки с большим кол-вом пропусков \n",
        "test_edited = test_data.drop(['Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
        "train_edited = train_data.drop(['Alley','FireplaceQu','PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
        "\n",
        "# Функция заменяет:\n",
        "# числовое отсутствующее значение значением median\n",
        "# строковое отсутствующее значение значением missing\n",
        "def nan_filler(data):\n",
        "    for label, content in data.items():\n",
        "        if pd.api.types.is_numeric_dtype(content):\n",
        "            data[label] = content.fillna(content.median())\n",
        "        else:\n",
        "            data[label] = content.astype(\"category\").cat.as_ordered()\n",
        "            data[label] = pd.Categorical(content).codes + 1\n",
        "\n",
        "nan_filler(test_edited)\n",
        "nan_filler(train_edited)"
      ],
      "execution_count":5,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Перепроверим наши данные:"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Проверяем наличие пустых значений в тестовой выборке\n",
        "missing_value_checker(test_edited)"
      ],
      "execution_count":6,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Empty DataFrame\n",
            "Columns: [name, empty_sum, type]\n",
            "Index: []\n",
            "Кол-во метрик с пустыми значениями 0\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "# Проверяем наличие пустых значений в тренировочной выборке\n",
        "missing_value_checker(train_edited)"
      ],
      "execution_count":7,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Empty DataFrame\n",
            "Columns: [name, empty_sum, type]\n",
            "Index: []\n",
            "Кол-во метрик с пустыми значениями 0\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Смотрим размеры тренировочных и тестовых данных\n",
        "train_edited.shape, test_edited.shape"
      ],
      "execution_count":8,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "((1460, 76), (1459, 75))"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "test_edited.info()"
      ],
      "execution_count":9,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1459 entries, 0 to 1458\n",
            "Data columns (total 75 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             1459 non-null   int64  \n",
            " 1   MSSubClass     1459 non-null   int64  \n",
            " 2   MSZoning       1459 non-null   int8   \n",
            " 3   LotFrontage    1459 non-null   float64\n",
            " 4   LotArea        1459 non-null   int64  \n",
            " 5   Street         1459 non-null   int8   \n",
            " 6   LotShape       1459 non-null   int8   \n",
            " 7   LandContour    1459 non-null   int8   \n",
            " 8   Utilities      1459 non-null   int8   \n",
            " 9   LotConfig      1459 non-null   int8   \n",
            " 10  LandSlope      1459 non-null   int8   \n",
            " 11  Neighborhood   1459 non-null   int8   \n",
            " 12  Condition1     1459 non-null   int8   \n",
            " 13  Condition2     1459 non-null   int8   \n",
            " 14  BldgType       1459 non-null   int8   \n",
            " 15  HouseStyle     1459 non-null   int8   \n",
            " 16  OverallQual    1459 non-null   int64  \n",
            " 17  OverallCond    1459 non-null   int64  \n",
            " 18  YearBuilt      1459 non-null   int64  \n",
            " 19  YearRemodAdd   1459 non-null   int64  \n",
            " 20  RoofStyle      1459 non-null   int8   \n",
            " 21  RoofMatl       1459 non-null   int8   \n",
            " 22  Exterior1st    1459 non-null   int8   \n",
            " 23  Exterior2nd    1459 non-null   int8   \n",
            " 24  MasVnrType     1459 non-null   int8   \n",
            " 25  MasVnrArea     1459 non-null   float64\n",
            " 26  ExterQual      1459 non-null   int8   \n",
            " 27  ExterCond      1459 non-null   int8   \n",
            " 28  Foundation     1459 non-null   int8   \n",
            " 29  BsmtQual       1459 non-null   int8   \n",
            " 30  BsmtCond       1459 non-null   int8   \n",
            " 31  BsmtExposure   1459 non-null   int8   \n",
            " 32  BsmtFinType1   1459 non-null   int8   \n",
            " 33  BsmtFinSF1     1459 non-null   float64\n",
            " 34  BsmtFinType2   1459 non-null   int8   \n",
            " 35  BsmtFinSF2     1459 non-null   float64\n",
            " 36  BsmtUnfSF      1459 non-null   float64\n",
            " 37  TotalBsmtSF    1459 non-null   float64\n",
            " 38  Heating        1459 non-null   int8   \n",
            " 39  HeatingQC      1459 non-null   int8   \n",
            " 40  CentralAir     1459 non-null   int8   \n",
            " 41  Electrical     1459 non-null   int8   \n",
            " 42  1stFlrSF       1459 non-null   int64  \n",
            " 43  2ndFlrSF       1459 non-null   int64  \n",
            " 44  LowQualFinSF   1459 non-null   int64  \n",
            " 45  GrLivArea      1459 non-null   int64  \n",
            " 46  BsmtFullBath   1459 non-null   float64\n",
            " 47  BsmtHalfBath   1459 non-null   float64\n",
            " 48  FullBath       1459 non-null   int64  \n",
            " 49  HalfBath       1459 non-null   int64  \n",
            " 50  BedroomAbvGr   1459 non-null   int64  \n",
            " 51  KitchenAbvGr   1459 non-null   int64  \n",
            " 52  KitchenQual    1459 non-null   int8   \n",
            " 53  TotRmsAbvGrd   1459 non-null   int64  \n",
            " 54  Functional     1459 non-null   int8   \n",
            " 55  Fireplaces     1459 non-null   int64  \n",
            " 56  GarageType     1459 non-null   int8   \n",
            " 57  GarageYrBlt    1459 non-null   float64\n",
            " 58  GarageFinish   1459 non-null   int8   \n",
            " 59  GarageCars     1459 non-null   float64\n",
            " 60  GarageArea     1459 non-null   float64\n",
            " 61  GarageQual     1459 non-null   int8   \n",
            " 62  GarageCond     1459 non-null   int8   \n",
            " 63  PavedDrive     1459 non-null   int8   \n",
            " 64  WoodDeckSF     1459 non-null   int64  \n",
            " 65  OpenPorchSF    1459 non-null   int64  \n",
            " 66  EnclosedPorch  1459 non-null   int64  \n",
            " 67  3SsnPorch      1459 non-null   int64  \n",
            " 68  ScreenPorch    1459 non-null   int64  \n",
            " 69  PoolArea       1459 non-null   int64  \n",
            " 70  MiscVal        1459 non-null   int64  \n",
            " 71  MoSold         1459 non-null   int64  \n",
            " 72  YrSold         1459 non-null   int64  \n",
            " 73  SaleType       1459 non-null   int8   \n",
            " 74  SaleCondition  1459 non-null   int8   \n",
            "dtypes: float64(11), int64(26), int8(38)\n",
            "memory usage: 476.0 KB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_edited.info()"
      ],
      "execution_count":10,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 76 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             1460 non-null   int64  \n",
            " 1   MSSubClass     1460 non-null   int64  \n",
            " 2   MSZoning       1460 non-null   int8   \n",
            " 3   LotFrontage    1460 non-null   float64\n",
            " 4   LotArea        1460 non-null   int64  \n",
            " 5   Street         1460 non-null   int8   \n",
            " 6   LotShape       1460 non-null   int8   \n",
            " 7   LandContour    1460 non-null   int8   \n",
            " 8   Utilities      1460 non-null   int8   \n",
            " 9   LotConfig      1460 non-null   int8   \n",
            " 10  LandSlope      1460 non-null   int8   \n",
            " 11  Neighborhood   1460 non-null   int8   \n",
            " 12  Condition1     1460 non-null   int8   \n",
            " 13  Condition2     1460 non-null   int8   \n",
            " 14  BldgType       1460 non-null   int8   \n",
            " 15  HouseStyle     1460 non-null   int8   \n",
            " 16  OverallQual    1460 non-null   int64  \n",
            " 17  OverallCond    1460 non-null   int64  \n",
            " 18  YearBuilt      1460 non-null   int64  \n",
            " 19  YearRemodAdd   1460 non-null   int64  \n",
            " 20  RoofStyle      1460 non-null   int8   \n",
            " 21  RoofMatl       1460 non-null   int8   \n",
            " 22  Exterior1st    1460 non-null   int8   \n",
            " 23  Exterior2nd    1460 non-null   int8   \n",
            " 24  MasVnrType     1460 non-null   int8   \n",
            " 25  MasVnrArea     1460 non-null   float64\n",
            " 26  ExterQual      1460 non-null   int8   \n",
            " 27  ExterCond      1460 non-null   int8   \n",
            " 28  Foundation     1460 non-null   int8   \n",
            " 29  BsmtQual       1460 non-null   int8   \n",
            " 30  BsmtCond       1460 non-null   int8   \n",
            " 31  BsmtExposure   1460 non-null   int8   \n",
            " 32  BsmtFinType1   1460 non-null   int8   \n",
            " 33  BsmtFinSF1     1460 non-null   int64  \n",
            " 34  BsmtFinType2   1460 non-null   int8   \n",
            " 35  BsmtFinSF2     1460 non-null   int64  \n",
            " 36  BsmtUnfSF      1460 non-null   int64  \n",
            " 37  TotalBsmtSF    1460 non-null   int64  \n",
            " 38  Heating        1460 non-null   int8   \n",
            " 39  HeatingQC      1460 non-null   int8   \n",
            " 40  CentralAir     1460 non-null   int8   \n",
            " 41  Electrical     1460 non-null   int8   \n",
            " 42  1stFlrSF       1460 non-null   int64  \n",
            " 43  2ndFlrSF       1460 non-null   int64  \n",
            " 44  LowQualFinSF   1460 non-null   int64  \n",
            " 45  GrLivArea      1460 non-null   int64  \n",
            " 46  BsmtFullBath   1460 non-null   int64  \n",
            " 47  BsmtHalfBath   1460 non-null   int64  \n",
            " 48  FullBath       1460 non-null   int64  \n",
            " 49  HalfBath       1460 non-null   int64  \n",
            " 50  BedroomAbvGr   1460 non-null   int64  \n",
            " 51  KitchenAbvGr   1460 non-null   int64  \n",
            " 52  KitchenQual    1460 non-null   int8   \n",
            " 53  TotRmsAbvGrd   1460 non-null   int64  \n",
            " 54  Functional     1460 non-null   int8   \n",
            " 55  Fireplaces     1460 non-null   int64  \n",
            " 56  GarageType     1460 non-null   int8   \n",
            " 57  GarageYrBlt    1460 non-null   float64\n",
            " 58  GarageFinish   1460 non-null   int8   \n",
            " 59  GarageCars     1460 non-null   int64  \n",
            " 60  GarageArea     1460 non-null   int64  \n",
            " 61  GarageQual     1460 non-null   int8   \n",
            " 62  GarageCond     1460 non-null   int8   \n",
            " 63  PavedDrive     1460 non-null   int8   \n",
            " 64  WoodDeckSF     1460 non-null   int64  \n",
            " 65  OpenPorchSF    1460 non-null   int64  \n",
            " 66  EnclosedPorch  1460 non-null   int64  \n",
            " 67  3SsnPorch      1460 non-null   int64  \n",
            " 68  ScreenPorch    1460 non-null   int64  \n",
            " 69  PoolArea       1460 non-null   int64  \n",
            " 70  MiscVal        1460 non-null   int64  \n",
            " 71  MoSold         1460 non-null   int64  \n",
            " 72  YrSold         1460 non-null   int64  \n",
            " 73  SaleType       1460 non-null   int8   \n",
            " 74  SaleCondition  1460 non-null   int8   \n",
            " 75  SalePrice      1460 non-null   int64  \n",
            "dtypes: float64(3), int64(35), int8(38)\n",
            "memory usage: 487.7 KB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Разделим данные\n",
        "\n",
        "Поскольку мы не знаем метку (Цена) тестовых данных, для оценки модели, чтобы получить лучшую модель перед прогнозированием тестовых данных, разделим данные в файле train.scv на обучающие и проверочные данные, соотношение составляет 20%."
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "X = train_edited.drop('SalePrice', axis=1)\n",
        "y = train_edited['SalePrice']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count":11,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "X_train.shape, test_edited.shape"
      ],
      "execution_count":12,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "((1168, 75), (1459, 75))"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Моделирование"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Создаем последовательную модель нейронной сети с помощью фрэймворка Tensorflow"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Построение и обучение первой модели"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import tensorflow as tf\n",
        "#Для обеспечения воспроизводимости результатов устанавливается функция seed\n",
        "tf.random.set_seed(40)\n",
        "\n",
        "# Модель Sequential представляет собой линейный стек слоев\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import activations"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# В ходе добавления нескольких слоев, была получена данная модель нейронной сети, МАЕ составила ~55000 (наилучший из возможных)\n",
        "\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(100))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(100))\n",
        "model.add(layers.Dropout(0.2))  # Чтобы нейросеть не переобучалась\n",
        "model.add(layers.Dense(100))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "# Выпадение - это техника, при которой случайно выбранные нейроны игнорируются во время тренировки. Они выбывают случайно. \n",
        "# Это означает, что их вклад в активацию нижестоящих нейронов временно удален на прямом проходе, и любые обновления веса не применяются к нейрону на обратном проходе"
      ],
      "execution_count":14,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "# Коомпилируем нейронную сеть, выбрав функцию потерь и оптимизатор.\n",
        "#Для оценки потерь используем MSLE(MeanSquaredLogarithmicError), а также метрику MAE(Mean absolute error).\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) "
      ],
      "execution_count":15,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Обучите модель на обучающих данных X_train и y_train задав гиперпараметры вашей модели нейронной сети, \n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=0)"
      ],
      "execution_count":16,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Оцениваем полученные результаты"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def plot_and_history(info_for_compare):\n",
        "    print(history.history)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.suptitle(info_for_compare)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['MAE'], color = 'g')\n",
        "    plt.title('MAE')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.xlim([0, len(history.history['MAE'])])\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('loss')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.xlim([0, len(history.history['loss'])])\n",
        "\n",
        "plot_and_history('Первая модель')"
      ],
      "execution_count":30,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "{'loss': [6.916408538818359, 0.32209309935569763, 0.19468821585178375, 0.18079856038093567, 0.1741115152835846, 0.1609029322862625, 0.14459732174873352, 0.1355023831129074, 0.12637268006801605, 0.1213897168636322], 'MAE': [144668.390625, 74503.1875, 64624.3359375, 62450.2734375, 61473.24609375, 60337.7734375, 56490.35546875, 54802.984375, 52658.33203125, 51263.015625]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "scores = model.evaluate(X_val, y_val, verbose=1)"
      ],
      "execution_count":23,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0862 - MAE: 38635.0898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0825 - MAE: 39406.7617\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Настраиваем параметры для получения лучших результатов"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Пробуем добавить функции активации на скрытые слои**<br>\n",
        "Функции активации нужны, чтобы проверить результирующее значение, создаваемое нейроном, и решить, должны ли внешние соединения рассматривать этот нейрон как «запущенный» или нет.<br>\n",
        "\n",
        "Сигмоидная или логистическая функция активации - не подходит для нашей задачи т.к переводит входные данные в диапазоне [-Inf; + Inf] к диапазону в (0; 1).\n",
        "Функция активации tanh - так же нам не подходит, т.к переводит входные данные  к диапазону в (-1; 1)<br>\n",
        "Функция активации softmax - сдавливает выходы каждого блока в диапазоне от 0 до 1, как сигмоид. Он также делит каждый выход так, что общая сумма выходов равна 1<br>\n",
        "Функция активации ReLU - если входное значение <0, то выходное значение 0, иначе выходное значение равно входному <br>\n"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#  С помошью добавления ф-ии активации relu уменьшили МАЕ до 39000\n",
        "model_2 = Sequential()\n",
        "# Входной слой\n",
        "model_2.add(layers.Dense(100))\n",
        "# Скрытые слои\n",
        "model_2.add(layers.Dense(100, activation=activations.relu))\n",
        "model_2.add(layers.Dropout(0.2)) \n",
        "model_2.add(layers.Dense(100))\n",
        "# Выходной слой\n",
        "model_2.add(layers.Dense(1))\n",
        "\n",
        "model_2.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "history = model_2.fit(X_train, y_train, epochs=10, batch_size=10, verbose=1)\n",
        "plot_and_history('Модель с функцией активации')\n",
        "scores = model_2.evaluate(X_val, y_val, verbose=1)"
      ],
      "execution_count":31,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Epoch 1\/10\n",
            "\r  1\/117 [..............................] - ETA: 59s - loss: 57.0032 - MAE: 196633.6094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 10.2174 - MAE: 168039.5156 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 57\/117 [=============>................] - ETA: 0s - loss: 6.3239 - MAE: 152668.2969 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 84\/117 [====================>.........] - ETA: 0s - loss: 4.5016 - MAE: 135181.0312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r112\/117 [===========================>..] - ETA: 0s - loss: 3.4282 - MAE: 116693.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 1s 2ms\/step - loss: 3.2973 - MAE: 115993.8750\n",
            "Epoch 2\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.1932 - MAE: 51534.9648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 0.1685 - MAE: 56252.7734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 58\/117 [=============>................] - ETA: 0s - loss: 0.1663 - MAE: 59444.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.1625 - MAE: 60706.5391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r114\/117 [============================>.] - ETA: 0s - loss: 0.1543 - MAE: 58760.3203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1557 - MAE: 59038.2344\n",
            "Epoch 3\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.1164 - MAE: 59957.0117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 0.1509 - MAE: 56699.6367\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 55\/117 [=============>................] - ETA: 0s - loss: 0.1376 - MAE: 53874.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 83\/117 [====================>.........] - ETA: 0s - loss: 0.1397 - MAE: 54716.8516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111\/117 [===========================>..] - ETA: 0s - loss: 0.1397 - MAE: 55923.5312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1443 - MAE: 57999.7734\n",
            "Epoch 4\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0601 - MAE: 29974.9648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 0.1235 - MAE: 51278.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 52\/117 [============>.................] - ETA: 0s - loss: 0.1467 - MAE: 59938.6406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 81\/117 [===================>..........] - ETA: 0s - loss: 0.1422 - MAE: 57286.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r106\/117 [==========================>...] - ETA: 0s - loss: 0.1439 - MAE: 57162.3867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1447 - MAE: 57404.9844\n",
            "Epoch 5\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0460 - MAE: 26114.6309\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 30\/117 [======>.......................] - ETA: 0s - loss: 0.1397 - MAE: 60760.2148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 57\/117 [=============>................] - ETA: 0s - loss: 0.1248 - MAE: 54100.4062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.1251 - MAE: 53985.0312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r116\/117 [============================>.] - ETA: 0s - loss: 0.1275 - MAE: 53794.8008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1271 - MAE: 53684.5352\n",
            "Epoch 6\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0764 - MAE: 33744.9727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 30\/117 [======>.......................] - ETA: 0s - loss: 0.1132 - MAE: 47804.9805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60\/117 [==============>...............] - ETA: 0s - loss: 0.1138 - MAE: 49108.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 90\/117 [======================>.......] - ETA: 0s - loss: 0.1203 - MAE: 52180.4922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1165 - MAE: 50489.6211\n",
            "Epoch 7\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.3021 - MAE: 93360.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 0.1196 - MAE: 53112.4570\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 59\/117 [==============>...............] - ETA: 0s - loss: 0.1188 - MAE: 53468.8594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.1159 - MAE: 50699.3828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r116\/117 [============================>.] - ETA: 0s - loss: 0.1114 - MAE: 49475.5391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1109 - MAE: 49369.8594\n",
            "Epoch 8\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0902 - MAE: 52709.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 29\/117 [======>.......................] - ETA: 0s - loss: 0.1050 - MAE: 45892.5352\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 59\/117 [==============>...............] - ETA: 0s - loss: 0.1092 - MAE: 47964.7578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88\/117 [=====================>........] - ETA: 0s - loss: 0.1052 - MAE: 48095.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0988 - MAE: 46279.7070\n",
            "Epoch 9\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.2023 - MAE: 95902.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 30\/117 [======>.......................] - ETA: 0s - loss: 0.1110 - MAE: 51124.9414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 59\/117 [==============>...............] - ETA: 0s - loss: 0.1007 - MAE: 46232.7891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88\/117 [=====================>........] - ETA: 0s - loss: 0.0982 - MAE: 45220.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - ETA: 0s - loss: 0.0932 - MAE: 43943.1250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0932 - MAE: 43943.1250\n",
            "Epoch 10\/10\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0672 - MAE: 37799.0078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 26\/117 [=====>........................] - ETA: 0s - loss: 0.1031 - MAE: 47168.8711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 56\/117 [=============>................] - ETA: 0s - loss: 0.0921 - MAE: 43632.4219\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0902 - MAE: 43562.1914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r116\/117 [============================>.] - ETA: 0s - loss: 0.0849 - MAE: 42417.1133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0847 - MAE: 42327.6562\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0716 - MAE: 32853.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0690 - MAE: 35021.4102\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Пробуем использовать разное количество нейронов на входном слое**"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Лучший результат МАЕ = 33000 получен при количестве нейронов на первом слое 250, поставим это значение в качестве основного\n",
        "def find_amount_of_neurons(amount):\n",
        "    model = Sequential()\n",
        "    # Входной слой\n",
        "    model.add(layers.Dense(amount))\n",
        "    # Скрытые слои\n",
        "    model.add(layers.Dense(100,activation=activations.relu))\n",
        "    model.add(layers.Dropout(0.2)) \n",
        "    model.add(layers.Dense(100))\n",
        "    # Выходной слой\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=0)\n",
        "    scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f'На первом слое {amount} нейронов')\n",
        "    print(scores)\n",
        "    plot_and_history(amount)\n",
        "\n",
        "list_of_neurons = [100, 150, 200, 250, 300]\n",
        "for i in list_of_neurons:\n",
        "    find_amount_of_neurons(i)"
      ],
      "execution_count":34,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0619 - MAE: 29886.1699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0592 - MAE: 32080.8223\n",
            "На первом слое 100 нейронов\n",
            "[0.05923028662800789, 32080.822265625]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0634 - MAE: 29916.5742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0576 - MAE: 31423.1309\n",
            "На первом слое 150 нейронов\n",
            "[0.0575985312461853, 31423.130859375]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0521 - MAE: 26082.4727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0514 - MAE: 29392.9492\n",
            "На первом слое 200 нейронов\n",
            "[0.05144469067454338, 29392.94921875]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0498 - MAE: 26915.3086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0489 - MAE: 29024.1035\n",
            "На первом слое 250 нейронов\n",
            "[0.048860568553209305, 29024.103515625]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0873 - MAE: 38365.1875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0883 - MAE: 40832.1172\n",
            "На первом слое 300 нейронов\n",
            "[0.08826438337564468, 40832.1171875]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Подбираем количесвто нейронов на скрытых слоях, чтобы улучшить показатели"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Лучший результат МАЕ = 30500 получен при количестве нейронов на втором слое 300, поставим это значение в качестве основного\n",
        "def find_amount_of_neurons(amount):\n",
        "    model = Sequential()\n",
        "    # Входной слой\n",
        "    model.add(layers.Dense(250))\n",
        "    # Скрытые слои\n",
        "    model.add(layers.Dense(amount,activation=activations.relu))\n",
        "    model.add(layers.Dropout(0.2)) \n",
        "    model.add(layers.Dense(100))\n",
        "    # Выходной слой\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=0)\n",
        "    scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f'На втором слое {amount} нейронов')\n",
        "    print(scores)\n",
        "    plot_and_history(amount)\n",
        "\n",
        "list_of_neurons = [100, 150, 200, 250, 300]\n",
        "for i in list_of_neurons:\n",
        "    find_amount_of_neurons(i)"
      ],
      "execution_count":201,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0535 - MAE: 34816.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0698 - MAE: 38442.0742\n",
            "На первом слое 100 нейронов\n",
            "[0.06978530436754227, 38442.07421875]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0573 - MAE: 35728.4414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0588 - MAE: 35070.5938\n",
            "На первом слое 150 нейронов\n",
            "[0.05883568897843361, 35070.59375]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0563 - MAE: 36505.6016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0758 - MAE: 39376.6172\n",
            "На первом слое 200 нейронов\n",
            "[0.07576382160186768, 39376.6171875]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0585 - MAE: 36744.2539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0535 - MAE: 33245.6719\n",
            "На первом слое 250 нейронов\n",
            "[0.05345930904150009, 33245.671875]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0537 - MAE: 34682.9141\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0491 - MAE: 30497.3086\n",
            "На первом слое 300 нейронов\n",
            "[0.049073390662670135, 30497.30859375]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Лучший результат МАЕ = 30000 получен при количестве нейронов на третьем слое 150, поставим это значение в качестве основного\n",
        "# Нейросеть стала дольше обучаться, при этом перестал происходить прирост точности\n",
        "def find_amount_of_neurons(amount):\n",
        "    model = Sequential()\n",
        "    # Входной слой\n",
        "    model.add(layers.Dense(250))\n",
        "    # Скрытые слои\n",
        "    model.add(layers.Dense(300,activation=activations.relu))\n",
        "    model.add(layers.Dropout(0.2)) \n",
        "    model.add(layers.Dense(amount))\n",
        "    # Выходной слой\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=0)\n",
        "    scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f'На третьем слое {amount} нейронов')\n",
        "    print(scores)\n",
        "    plot_and_history(amount)\n",
        "\n",
        "list_of_neurons = [100, 150, 200, 250, 300]\n",
        "for i in list_of_neurons:\n",
        "    find_amount_of_neurons(i)"
      ],
      "execution_count":202,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0565 - MAE: 35542.7969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0496 - MAE: 31125.2148\n",
            "На третьем слое 100 нейронов\n",
            "[0.04961702972650528, 31125.21484375]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0521 - MAE: 33397.3906\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0495 - MAE: 30088.7012\n",
            "На третьем слое 150 нейронов\n",
            "[0.04948701336979866, 30088.701171875]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0504 - MAE: 33306.8906\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0512 - MAE: 30719.1445\n",
            "На третьем слое 200 нейронов\n",
            "[0.05117577314376831, 30719.14453125]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0513 - MAE: 34124.5117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0488 - MAE: 30505.8594\n",
            "На третьем слое 250 нейронов\n",
            "[0.048804398626089096, 30505.859375]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0504 - MAE: 34400.3320\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0542 - MAE: 32570.5957\n",
            "На третьем слое 300 нейронов\n",
            "[0.0542418472468853, 32570.595703125]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        " **Используем разное количество эпох**<br>\n",
        " Произошла одна эпоха (epoch) — весь датасет прошел через нейронную сеть в прямом и обратном направлении только один раз."
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# 30 эпох дают лучший результат. МАЕ = 27500\n",
        "def find_amount_of_epochs(amount):\n",
        "    model = Sequential()\n",
        "    # Входной слой\n",
        "    model.add(layers.Dense(250))\n",
        "    # Скрытые слои\n",
        "    model.add(layers.Dense(300,activation=activations.relu))\n",
        "    model.add(layers.Dropout(0.2)) \n",
        "    model.add(layers.Dense(150))\n",
        "    # Выходной слой\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "    history = model.fit(X_train, y_train, epochs=amount, batch_size=10, verbose=0)\n",
        "    scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f'Кол-во эпох: {amount} ')\n",
        "    print(scores)\n",
        "    plot_and_history(amount)\n",
        "\n",
        "list_of_epochs = [10, 15, 20, 25, 30]\n",
        "for i in list_of_epochs:\n",
        "    find_amount_of_epochs(i)"
      ],
      "execution_count":33,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0438 - MAE: 28133.2910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0434 - MAE: 28865.8047\n",
            "Кол-во эпох: 10 \n",
            "[0.04341979697346687, 28865.8046875]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0410 - MAE: 24647.8320\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0404 - MAE: 26357.8047\n",
            "Кол-во эпох: 15 \n",
            "[0.04038219153881073, 26357.8046875]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0376 - MAE: 21904.8555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0387 - MAE: 24161.3145\n",
            "Кол-во эпох: 20 \n",
            "[0.038666900247335434, 24161.314453125]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0478 - MAE: 23098.3965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0446 - MAE: 25157.3574\n",
            "Кол-во эпох: 25 \n",
            "[0.04463829845190048, 25157.357421875]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0408 - MAE: 26337.3477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0402 - MAE: 27168.1465\n",
            "Кол-во эпох: 30 \n",
            "[0.04023689031600952, 27168.146484375]\n",
            "{'loss': [3.2973053455352783, 0.1556895226240158, 0.14425818622112274, 0.14473235607147217, 0.12714380025863647, 0.11648599058389664, 0.11092182993888855, 0.09877196699380875, 0.09316609799861908, 0.08465676009654999], 'MAE': [115993.875, 59038.234375, 57999.7734375, 57404.984375, 53684.53515625, 50489.62109375, 49369.859375, 46279.70703125, 43943.125, 42327.65625]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Используем разные размеры мини-выборки (batch_size)**<br>\n",
        "Нельзя пропустить через нейронную сеть разом весь датасет. Поэтому делим данные на небольшие партии<br>\n",
        "batch_size - размер пакета, т.е кол-во элементов нашей партии<br>\n",
        "Итерации — число батчей, необходимых для завершения одной эпохи(сколько полных батчей помещается в наш датасет)<br>"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Лучший результат остался при batch_size = 10\n",
        "def find_batch_size(amount):\n",
        "    model = Sequential()\n",
        "    # Входной слой\n",
        "    model.add(layers.Dense(250))\n",
        "    # Скрытые слои\n",
        "    model.add(layers.Dense(300,activation=activations.relu))\n",
        "    model.add(layers.Dropout(0.2)) \n",
        "    model.add(layers.Dense(150))\n",
        "    # Выходной слой\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE']) \n",
        "    history = model.fit(X_train, y_train, epochs=30, batch_size=amount, verbose=0)\n",
        "    scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "    print(f'Размер партии: {amount} ')\n",
        "    print(scores)\n",
        "    plot_and_history(amount)\n",
        "\n",
        "list_of_batch_size = [10, 15, 20, 25, 30]\n",
        "for i in list_of_batch_size:\n",
        "    find_batch_size(i)"
      ],
      "execution_count":205,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0413 - MAE: 31103.5547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0468 - MAE: 28255.9570\n",
            "Размер партии: 10 \n",
            "[0.046760641038417816, 28255.95703125]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0429 - MAE: 31035.8281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0514 - MAE: 29902.9004\n",
            "Размер партии: 15 \n",
            "[0.05142075568437576, 29902.900390625]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0498 - MAE: 33572.8281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0525 - MAE: 31305.7949\n",
            "Размер партии: 20 \n",
            "[0.052459705621004105, 31305.794921875]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0455 - MAE: 32287.2129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0478 - MAE: 28910.7734\n",
            "Размер партии: 25 \n",
            "[0.047835592180490494, 28910.7734375]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0487 - MAE: 33927.0547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0505 - MAE: 30865.8828\n",
            "Размер партии: 30 \n",
            "[0.05051755905151367, 30865.8828125]\n",
            "{'loss': [3.824916362762451, 0.16336092352867126, 0.1491982638835907, 0.13419829308986664, 0.13197246193885803, 0.11467784643173218, 0.10217291116714478, 0.09875570982694626, 0.09493336826562881, 0.08702591806650162], 'MAE': [117183.4140625, 58743.51953125, 56288.1015625, 54019.25390625, 52018.0859375, 48040.11328125, 45776.01953125, 44169.15234375, 43297.0390625, 41718.23828125]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Пробуем менять оптимизаторы**<br>\n",
        "SGD - стохастический градиентный спуск.Метод нахождения минимального значения функции.Поиск минимума означает получение наименьшей возможной ошибки <br>\n",
        "RMSprop - Делит скорость обучения для веса на скользящее среднее значение последних градиентов этого веса <br>\n",
        "Adagrad - скорость обучения параметра (веса) зависит от частоты его обновления: чем чаще обновляется параметр, тем меньше скорость его обучения <br>\n",
        "Adam - Aдаптивная оценка момента. вычисляет адаптивные скорости обучения для каждого параметра <br>\n",
        "\n",
        "Модель с оптимизатором Adam показала лучшую точность МАЕ = 28000( возможно это связано с тем, что все предыдущие настройки производились с данным оптимизатором)"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# SGD\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='sgd', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем оптимизатор: SGD ')\n",
        "print(scores)\n",
        "plot_and_history('SGD')\n",
        "print(history)"
      ],
      "execution_count":38,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.1044 - MAE: 48876.3984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0914 - MAE: 43585.8008\n",
            "Используем оптимизатор: SGD \n",
            "[0.09137977659702301, 43585.80078125]\n",
            "{'loss': [0.7365202307701111, 0.182735413312912, 0.17367951571941376, 0.17277710139751434, 0.16830402612686157, 0.16457298398017883, 0.16474536061286926, 0.15669496357440948, 0.158038929104805, 0.15750440955162048, 0.1545025259256363, 0.14931821823120117, 0.1481282114982605, 0.14470431208610535, 0.14392811059951782, 0.14205937087535858, 0.13753697276115417, 0.13114619255065918, 0.1345379650592804, 0.13464365899562836, 0.13105423748493195, 0.13049554824829102, 0.125507190823555, 0.12286340445280075, 0.12600581347942352, 0.12245464324951172, 0.11875634640455246, 0.12080156058073044, 0.1173979714512825, 0.11758115887641907], 'MAE': [84641.3046875, 64206.609375, 63166.546875, 62770.69140625, 62365.13671875, 60929.47265625, 61806.54296875, 59565.92578125, 60089.15625, 60215.1171875, 59628.671875, 58182.34765625, 58190.53515625, 57337.09765625, 57199.359375, 56966.62890625, 55781.0625, 54091.359375, 55387.55859375, 55183.78125, 54041.3359375, 53761.9609375, 53037.01171875, 52679.57421875, 52862.80078125, 52061.328125, 51071.8828125, 51228.60546875, 50737.59765625, 50500.30078125]}\n",
            "<keras.callbacks.History object at 0x7f9d10d66cd0>\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#Adagrad\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adagrad', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем оптимизатор: Adagrad ')\n",
        "print(scores)\n",
        "plot_and_history('Adagrad')\n",
        "print(history)"
      ],
      "execution_count":39,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.1355 - MAE: 52988.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.1398 - MAE: 53761.8789\n",
            "Используем оптимизатор: Adagrad \n",
            "[0.1398162543773651, 53761.87890625]\n",
            "{'loss': [9.339327812194824, 2.9627296924591064, 1.8276572227478027, 1.2653982639312744, 0.9313598871231079, 0.7208945751190186, 0.5833229422569275, 0.47915294766426086, 0.40273231267929077, 0.3492892384529114, 0.3085964322090149, 0.2768022418022156, 0.25084564089775085, 0.23522473871707916, 0.21699993312358856, 0.20870091021060944, 0.19365672767162323, 0.1873876452445984, 0.1844240427017212, 0.17709828913211823, 0.17762798070907593, 0.1718282401561737, 0.16798967123031616, 0.16458623111248016, 0.16553005576133728, 0.16497890651226044, 0.1609351933002472, 0.16011615097522736, 0.1622757911682129, 0.162178173661232], 'MAE': [166088.421875, 147494.15625, 133299.125, 121130.109375, 110835.90625, 101850.59375, 95073.8828125, 88866.1015625, 82797.6953125, 78374.234375, 74399.140625, 71641.9140625, 69076.4921875, 67696.859375, 65518.20703125, 63991.91796875, 62667.71875, 62026.9375, 62210.8203125, 61737.1640625, 61209.13671875, 60478.609375, 59867.19140625, 59537.31640625, 59907.71875, 59933.95703125, 59902.84375, 59297.08984375, 60139.5703125, 59847.71875]}\n",
            "<keras.callbacks.History object at 0x7f9d2c626e80>\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#RMSprop\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='rmsprop', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем оптимизатор: RMSprop ')\n",
        "print(scores)\n",
        "plot_and_history('RMSprop')\n",
        "print(history)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#Adam\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем оптимизатор: Adam ')\n",
        "print(scores)\n",
        "plot_and_history('Adam')"
      ],
      "execution_count":41,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0311 - MAE: 25610.4746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 0.0517 - MAE: 28608.7754\n",
            "Используем оптимизатор: Adam \n",
            "[0.05174512788653374, 28608.775390625]\n",
            "{'loss': [2.180927276611328, 0.12093038111925125, 0.09747743606567383, 0.07735186815261841, 0.0618961863219738, 0.05371647700667381, 0.050372328609228134, 0.05049555376172066, 0.051197003573179245, 0.047919947654008865, 0.04611522704362869, 0.04820195212960243, 0.045885391533374786, 0.04688260704278946, 0.045824021100997925, 0.046661924570798874, 0.04604313150048256, 0.0436118021607399, 0.043487172573804855, 0.04505639895796776, 0.04429551213979721, 0.04397166147828102, 0.04345058649778366, 0.04529646039009094, 0.045164305716753006, 0.04338157922029495, 0.043530482798814774, 0.04354935139417648, 0.04325661435723305, 0.04171978682279587], 'MAE': [82963.1640625, 51693.58984375, 45587.91015625, 39839.46875, 35289.23828125, 32329.77734375, 31192.25390625, 30983.845703125, 31374.228515625, 30209.626953125, 29786.2265625, 30345.076171875, 28967.44921875, 29330.353515625, 28784.537109375, 29127.345703125, 29199.828125, 27735.033203125, 28411.51171875, 28602.5078125, 28339.89453125, 27893.94921875, 28105.423828125, 28078.580078125, 29235.431640625, 27963.298828125, 28008.572265625, 28170.111328125, 27768.91015625, 27784.28515625]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Пробуем разные функции потерь**<br>\n",
        "Функция потерь MSLE - показала лучший результат (Возможно повлияло то, что все предыдущие параметры настраивалиьс при данной функции потерь)"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# MSLE - использовалась для обучения всех предыдущих моделей\n",
        "# Средняя квадратичная логарифмическая ошибка \n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем функцию потерь MSLE ')\n",
        "print(scores)\n",
        "plot_and_history('MSLE')"
      ],
      "execution_count":48,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0339 - MAE: 26494.6367\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0492 - MAE: 28667.4375\n",
            "Используем функцию потерь MSLE \n",
            "[0.04921257495880127, 28667.4375]\n",
            "{'loss': [1.6056684255599976, 0.14083534479141235, 0.11425089091062546, 0.09671857208013535, 0.07660437375307083, 0.06250405311584473, 0.05677688866853714, 0.05223247781395912, 0.0526583194732666, 0.0534987635910511, 0.0495191290974617, 0.050251517444849014, 0.04988187551498413, 0.04887447506189346, 0.050457995384931564, 0.05131012946367264, 0.04947328194975853, 0.04700044170022011, 0.04790506511926651, 0.04539065062999725, 0.04751739278435707, 0.04717138037085533, 0.04390977323055267, 0.04541782662272453, 0.0436207577586174, 0.044489819556474686, 0.0437406562268734, 0.0459488220512867, 0.0457787923514843, 0.043918315321207047], 'MAE': [89266.7421875, 56311.921875, 49866.15625, 45319.40234375, 39776.2734375, 35485.65234375, 33402.2109375, 32055.177734375, 32246.33203125, 32353.71484375, 31089.267578125, 31622.99609375, 30888.00390625, 30257.64453125, 30814.47265625, 30929.158203125, 30847.9375, 29170.541015625, 30111.708984375, 28643.97265625, 29365.1953125, 29355.7734375, 28159.384765625, 28702.46484375, 28165.396484375, 28336.751953125, 28223.287109375, 29137.658203125, 29167.064453125, 28111.689453125]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# MeanAbsoluteError - Средняя абсолютная ошибка \n",
        "# это мера того, насколько полученные моделью значения отличаются от истинных\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanAbsoluteError(), optimizer='adam', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем функцию потерь MАE ')\n",
        "print(scores)\n",
        "plot_and_history('MAE')"
      ],
      "execution_count":47,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 28793.4062 - MAE: 28793.4062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 30616.1875 - MAE: 30616.1875\n",
            "Используем функцию потерь MАE \n",
            "[30616.1875, 30616.1875]\n",
            "{'loss': [62163.59765625, 36393.82421875, 31719.0625, 30641.818359375, 31382.544921875, 29383.3671875, 30549.763671875, 30083.068359375, 28040.978515625, 28746.482421875, 28358.7109375, 27894.125, 27533.66796875, 27407.09765625, 26656.52734375, 26654.01953125, 28472.416015625, 26332.48046875, 27853.484375, 26659.240234375, 26811.142578125, 27259.330078125, 25887.744140625, 25399.21484375, 26244.66796875, 25343.763671875, 26593.017578125, 26109.634765625, 26213.689453125, 25624.201171875], 'MAE': [62163.59765625, 36393.82421875, 31719.0625, 30641.814453125, 31382.544921875, 29383.3671875, 30549.763671875, 30083.068359375, 28040.978515625, 28746.482421875, 28358.7109375, 27894.12109375, 27533.66796875, 27407.09765625, 26656.52734375, 26654.01953125, 28472.416015625, 26332.48046875, 27853.486328125, 26659.240234375, 26811.142578125, 27259.330078125, 25887.744140625, 25399.21484375, 26244.66796875, 25343.763671875, 26593.017578125, 26109.634765625, 26213.689453125, 25624.201171875]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Средняя квадратная ошибка (Mean Squared Error)  \n",
        "# это мера того, насколько квадраты полученных моделью значений отличается от истинных\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredError(), optimizer='adam', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=0)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f'Используем функцию потерь MSE ')\n",
        "print(scores)\n",
        "plot_and_history('MSE')"
      ],
      "execution_count":49,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 1518016512.0000 - MAE: 25585.1719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 2ms\/step - loss: 7574959616.0000 - MAE: 30357.3223\n",
            "Используем функцию потерь MSE \n",
            "[7574959616.0, 30357.322265625]\n",
            "{'loss': [12713562112.0, 4168375040.0, 2775363840.0, 2055171456.0, 1788731136.0, 1738568192.0, 1816988800.0, 1759361792.0, 1601069184.0, 1819124096.0, 1976518016.0, 1605179904.0, 1644567040.0, 1586574464.0, 1462604416.0, 1526306688.0, 1597982976.0, 1769081344.0, 1576881792.0, 1378580992.0, 1416018048.0, 1611869312.0, 1512003328.0, 1324259712.0, 1387521664.0, 1316750720.0, 1417426176.0, 1436818432.0, 1386349696.0, 1358905984.0], 'MAE': [70688.2265625, 45281.9140625, 36914.9140625, 31401.60546875, 29703.130859375, 29679.3671875, 29737.109375, 30242.9140625, 28421.46484375, 29716.80859375, 29648.103515625, 28254.671875, 28474.138671875, 28417.080078125, 27187.7578125, 28273.6796875, 27657.90625, 28551.69140625, 27993.48828125, 26232.876953125, 26646.447265625, 28171.353515625, 27354.90234375, 25737.375, 26567.4765625, 25999.146484375, 26686.130859375, 27078.78515625, 26553.08984375, 26083.408203125]}\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Предсказание"
      ],
      "attachments":{
        
      },
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"MD",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "#  Финальная модель, полученная в ходе лабораторной работы\n",
        "model = Sequential()\n",
        "# Входной слой\n",
        "model.add(layers.Dense(250))\n",
        "# Скрытые слои\n",
        "model.add(layers.Dense(300, activation=activations.relu))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(150))\n",
        "# Выходной слой\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(loss=losses.MeanSquaredLogarithmicError(), optimizer='adam', metrics=['MAE'])\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=10, verbose=1)\n",
        "scores = model.evaluate(X_val, y_val, verbose=1)"
      ],
      "execution_count":50,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Epoch 1\/30\n",
            "\r  1\/117 [..............................] - ETA: 1:02 - loss: 70.9546 - MAE: 189940.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 8.2850 - MAE: 162845.1562   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 4.7393 - MAE: 131228.1406\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 61\/117 [==============>...............] - ETA: 0s - loss: 3.2389 - MAE: 106295.7344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 79\/117 [===================>..........] - ETA: 0s - loss: 2.5331 - MAE: 94123.4297 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100\/117 [========================>.....] - ETA: 0s - loss: 2.0356 - MAE: 87633.9141\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 1s 3ms\/step - loss: 1.7670 - MAE: 85234.6641\n",
            "Epoch 2\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.1690 - MAE: 54429.3984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.1657 - MAE: 59107.8711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 42\/117 [=========>....................] - ETA: 0s - loss: 0.1416 - MAE: 54375.1719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.1304 - MAE: 53009.4336\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.1257 - MAE: 51850.3984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r109\/117 [==========================>...] - ETA: 0s - loss: 0.1245 - MAE: 52339.5156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.1260 - MAE: 53097.2930\n",
            "Epoch 3\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0801 - MAE: 32982.9258\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0902 - MAE: 42457.8945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.1079 - MAE: 48111.5039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 66\/117 [===============>..............] - ETA: 0s - loss: 0.1043 - MAE: 47507.5742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 77\/117 [==================>...........] - ETA: 0s - loss: 0.1083 - MAE: 47818.9297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.1031 - MAE: 46488.8789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r104\/117 [=========================>....] - ETA: 0s - loss: 0.0983 - MAE: 45669.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0991 - MAE: 45580.7891\n",
            "Epoch 4\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0714 - MAE: 46001.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0702 - MAE: 34612.3633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 39\/117 [=========>....................] - ETA: 0s - loss: 0.0806 - MAE: 41083.1250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 59\/117 [==============>...............] - ETA: 0s - loss: 0.0777 - MAE: 39797.9414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 78\/117 [===================>..........] - ETA: 0s - loss: 0.0751 - MAE: 39556.2344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 99\/117 [========================>.....] - ETA: 0s - loss: 0.0748 - MAE: 39478.0938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0748 - MAE: 39154.8672\n",
            "Epoch 5\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.1229 - MAE: 50299.4688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 21\/117 [====>.........................] - ETA: 0s - loss: 0.0649 - MAE: 36368.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0638 - MAE: 36626.3672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63\/117 [===============>..............] - ETA: 0s - loss: 0.0598 - MAE: 35432.5234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 85\/117 [====================>.........] - ETA: 0s - loss: 0.0594 - MAE: 35612.2852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r107\/117 [==========================>...] - ETA: 0s - loss: 0.0616 - MAE: 35662.8672\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0608 - MAE: 35192.0234\n",
            "Epoch 6\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0532 - MAE: 31593.5195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 24\/117 [=====>........................] - ETA: 0s - loss: 0.0430 - MAE: 28710.9590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 45\/117 [==========>...................] - ETA: 0s - loss: 0.0517 - MAE: 31875.8398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 68\/117 [================>.............] - ETA: 0s - loss: 0.0547 - MAE: 31997.6992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 89\/117 [=====================>........] - ETA: 0s - loss: 0.0537 - MAE: 32458.0195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111\/117 [===========================>..] - ETA: 0s - loss: 0.0549 - MAE: 33150.3945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0549 - MAE: 32997.6523\n",
            "Epoch 7\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0771 - MAE: 41154.5977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 21\/117 [====>.........................] - ETA: 0s - loss: 0.0399 - MAE: 29984.8301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0507 - MAE: 31943.8594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60\/117 [==============>...............] - ETA: 0s - loss: 0.0522 - MAE: 32033.9609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 81\/117 [===================>..........] - ETA: 0s - loss: 0.0533 - MAE: 31898.1797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r101\/117 [========================>.....] - ETA: 0s - loss: 0.0506 - MAE: 31077.3379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0529 - MAE: 31694.8594\n",
            "Epoch 8\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0583 - MAE: 42832.1797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0490 - MAE: 31180.5605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0532 - MAE: 30764.0527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62\/117 [==============>...............] - ETA: 0s - loss: 0.0530 - MAE: 30708.0488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 83\/117 [====================>.........] - ETA: 0s - loss: 0.0524 - MAE: 30955.0996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r104\/117 [=========================>....] - ETA: 0s - loss: 0.0522 - MAE: 30888.4551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0508 - MAE: 31024.6191\n",
            "Epoch 9\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0465 - MAE: 31471.1055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 21\/117 [====>.........................] - ETA: 0s - loss: 0.0531 - MAE: 33879.6914\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0524 - MAE: 32490.6992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 61\/117 [==============>...............] - ETA: 0s - loss: 0.0508 - MAE: 31820.5977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 82\/117 [====================>.........] - ETA: 0s - loss: 0.0483 - MAE: 30998.7070\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r101\/117 [========================>.....] - ETA: 0s - loss: 0.0513 - MAE: 31413.3945\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r115\/117 [============================>.] - ETA: 0s - loss: 0.0517 - MAE: 31398.6543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0519 - MAE: 31443.8418\n",
            "Epoch 10\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0200 - MAE: 22083.7383\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0585 - MAE: 35323.8242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 39\/117 [=========>....................] - ETA: 0s - loss: 0.0560 - MAE: 32323.7871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 58\/117 [=============>................] - ETA: 0s - loss: 0.0569 - MAE: 33069.7695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 78\/117 [===================>..........] - ETA: 0s - loss: 0.0547 - MAE: 32216.1445\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 98\/117 [========================>.....] - ETA: 0s - loss: 0.0541 - MAE: 31932.2656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0512 - MAE: 31005.9629\n",
            "Epoch 11\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0333 - MAE: 27317.1973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 19\/117 [===>..........................] - ETA: 0s - loss: 0.0371 - MAE: 26517.3965\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 38\/117 [========>.....................] - ETA: 0s - loss: 0.0397 - MAE: 28473.1680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 56\/117 [=============>................] - ETA: 0s - loss: 0.0427 - MAE: 29391.6387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 76\/117 [==================>...........] - ETA: 0s - loss: 0.0474 - MAE: 30283.5547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 97\/117 [=======================>......] - ETA: 0s - loss: 0.0470 - MAE: 30319.5918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0481 - MAE: 30053.4863\n",
            "Epoch 12\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0171 - MAE: 19745.9414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0656 - MAE: 35366.7383\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0573 - MAE: 32699.8262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64\/117 [===============>..............] - ETA: 0s - loss: 0.0531 - MAE: 31093.8535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0522 - MAE: 30962.8535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r107\/117 [==========================>...] - ETA: 0s - loss: 0.0519 - MAE: 31178.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0514 - MAE: 31261.4414\n",
            "Epoch 13\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0221 - MAE: 23875.7832\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 18\/117 [===>..........................] - ETA: 0s - loss: 0.0396 - MAE: 24759.1270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 39\/117 [=========>....................] - ETA: 0s - loss: 0.0471 - MAE: 29209.3867\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 58\/117 [=============>................] - ETA: 0s - loss: 0.0473 - MAE: 29930.3516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 79\/117 [===================>..........] - ETA: 0s - loss: 0.0486 - MAE: 29478.4258\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 99\/117 [========================>.....] - ETA: 0s - loss: 0.0482 - MAE: 30193.9004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0482 - MAE: 30083.0547\n",
            "Epoch 14\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0230 - MAE: 20617.4004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0481 - MAE: 28687.1621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 42\/117 [=========>....................] - ETA: 0s - loss: 0.0502 - MAE: 30212.2578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64\/117 [===============>..............] - ETA: 0s - loss: 0.0495 - MAE: 29874.6289\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0467 - MAE: 29351.9590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r107\/117 [==========================>...] - ETA: 0s - loss: 0.0457 - MAE: 29542.3691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0465 - MAE: 29463.0645\n",
            "Epoch 15\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0275 - MAE: 24513.3711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0470 - MAE: 28615.5957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0451 - MAE: 28990.5820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0451 - MAE: 28738.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 85\/117 [====================>.........] - ETA: 0s - loss: 0.0449 - MAE: 28894.8047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r103\/117 [=========================>....] - ETA: 0s - loss: 0.0459 - MAE: 28992.6816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0468 - MAE: 29108.2637\n",
            "Epoch 16\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0609 - MAE: 26593.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0477 - MAE: 28493.8691\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0477 - MAE: 28539.1309\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 66\/117 [===============>..............] - ETA: 0s - loss: 0.0488 - MAE: 29355.0820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0493 - MAE: 30195.7227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108\/117 [==========================>...] - ETA: 0s - loss: 0.0509 - MAE: 30831.1660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0510 - MAE: 30701.8770\n",
            "Epoch 17\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0103 - MAE: 16610.5254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0518 - MAE: 29698.8633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 40\/117 [=========>....................] - ETA: 0s - loss: 0.0515 - MAE: 28344.2148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62\/117 [==============>...............] - ETA: 0s - loss: 0.0502 - MAE: 30164.5996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 82\/117 [====================>.........] - ETA: 0s - loss: 0.0478 - MAE: 29311.2656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r105\/117 [=========================>....] - ETA: 0s - loss: 0.0485 - MAE: 29590.2305\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0480 - MAE: 29639.2988\n",
            "Epoch 18\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0284 - MAE: 28647.6992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 21\/117 [====>.........................] - ETA: 0s - loss: 0.0485 - MAE: 32699.3184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0489 - MAE: 30633.5879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64\/117 [===============>..............] - ETA: 0s - loss: 0.0461 - MAE: 29278.6602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 85\/117 [====================>.........] - ETA: 0s - loss: 0.0472 - MAE: 29092.0840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r106\/117 [==========================>...] - ETA: 0s - loss: 0.0464 - MAE: 29109.5840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0453 - MAE: 28863.1504\n",
            "Epoch 19\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0581 - MAE: 45911.3242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0471 - MAE: 30833.9297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0459 - MAE: 30512.6641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0447 - MAE: 29784.0156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0491 - MAE: 30418.8926\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108\/117 [==========================>...] - ETA: 0s - loss: 0.0466 - MAE: 29878.6934\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0460 - MAE: 29470.2500\n",
            "Epoch 20\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0201 - MAE: 18312.8535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0505 - MAE: 33767.2188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0429 - MAE: 30117.4023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63\/117 [===============>..............] - ETA: 0s - loss: 0.0420 - MAE: 29048.0156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 82\/117 [====================>.........] - ETA: 0s - loss: 0.0417 - MAE: 28204.2090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r104\/117 [=========================>....] - ETA: 0s - loss: 0.0442 - MAE: 27984.4961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0430 - MAE: 27716.9668\n",
            "Epoch 21\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.2274 - MAE: 50239.1797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0575 - MAE: 31121.9355\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0480 - MAE: 29004.6348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0439 - MAE: 28133.2734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0442 - MAE: 28179.4180\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108\/117 [==========================>...] - ETA: 0s - loss: 0.0430 - MAE: 27816.7344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0439 - MAE: 28144.8535\n",
            "Epoch 22\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0711 - MAE: 44422.0898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0585 - MAE: 32867.9531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0516 - MAE: 31756.7930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 67\/117 [================>.............] - ETA: 0s - loss: 0.0508 - MAE: 30462.8887\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88\/117 [=====================>........] - ETA: 0s - loss: 0.0482 - MAE: 30221.1230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111\/117 [===========================>..] - ETA: 0s - loss: 0.0472 - MAE: 29396.0918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0469 - MAE: 29277.5312\n",
            "Epoch 23\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0574 - MAE: 33924.4805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 21\/117 [====>.........................] - ETA: 0s - loss: 0.0531 - MAE: 31272.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0468 - MAE: 29164.2422\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64\/117 [===============>..............] - ETA: 0s - loss: 0.0456 - MAE: 28170.1035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 83\/117 [====================>.........] - ETA: 0s - loss: 0.0428 - MAE: 27956.6387\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100\/117 [========================>.....] - ETA: 0s - loss: 0.0436 - MAE: 27909.7129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 3ms\/step - loss: 0.0430 - MAE: 27606.5586\n",
            "Epoch 24\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0406 - MAE: 30012.1973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 20\/117 [====>.........................] - ETA: 0s - loss: 0.0499 - MAE: 28613.6797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 41\/117 [=========>....................] - ETA: 0s - loss: 0.0506 - MAE: 30217.7246\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62\/117 [==============>...............] - ETA: 0s - loss: 0.0444 - MAE: 28560.9199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 84\/117 [====================>.........] - ETA: 0s - loss: 0.0427 - MAE: 27910.9473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r104\/117 [=========================>....] - ETA: 0s - loss: 0.0428 - MAE: 27925.0664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0430 - MAE: 27890.0293\n",
            "Epoch 25\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.1351 - MAE: 45020.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0509 - MAE: 30703.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 42\/117 [=========>....................] - ETA: 0s - loss: 0.0449 - MAE: 28667.7090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63\/117 [===============>..............] - ETA: 0s - loss: 0.0431 - MAE: 28055.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 84\/117 [====================>.........] - ETA: 0s - loss: 0.0414 - MAE: 27390.3281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r105\/117 [=========================>....] - ETA: 0s - loss: 0.0440 - MAE: 28217.2109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0440 - MAE: 28008.8652\n",
            "Epoch 26\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0255 - MAE: 29167.9746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0522 - MAE: 30614.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0434 - MAE: 28627.4004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0424 - MAE: 28212.8301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.0421 - MAE: 27553.7480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r109\/117 [==========================>...] - ETA: 0s - loss: 0.0440 - MAE: 27847.6855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0443 - MAE: 27941.4922\n",
            "Epoch 27\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0301 - MAE: 27665.8066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0353 - MAE: 27572.8105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 43\/117 [==========>...................] - ETA: 0s - loss: 0.0411 - MAE: 27203.7246\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0425 - MAE: 27699.1699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0431 - MAE: 27788.5527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108\/117 [==========================>...] - ETA: 0s - loss: 0.0429 - MAE: 27735.2500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0433 - MAE: 27716.3984\n",
            "Epoch 28\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0265 - MAE: 25304.3086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0453 - MAE: 27217.0098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0491 - MAE: 27452.5742\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0437 - MAE: 26913.3633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 86\/117 [=====================>........] - ETA: 0s - loss: 0.0449 - MAE: 27947.3652\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r107\/117 [==========================>...] - ETA: 0s - loss: 0.0445 - MAE: 28171.7441\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0448 - MAE: 28562.0566\n",
            "Epoch 29\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0208 - MAE: 28742.9629\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 22\/117 [====>.........................] - ETA: 0s - loss: 0.0461 - MAE: 31044.2324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0431 - MAE: 28608.4043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65\/117 [===============>..............] - ETA: 0s - loss: 0.0462 - MAE: 28634.8496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 88\/117 [=====================>........] - ETA: 0s - loss: 0.0476 - MAE: 29350.9941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r110\/117 [===========================>..] - ETA: 0s - loss: 0.0448 - MAE: 28284.8809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0447 - MAE: 28404.1875\n",
            "Epoch 30\/30\n",
            "\r  1\/117 [..............................] - ETA: 0s - loss: 0.0322 - MAE: 26944.7715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 23\/117 [====>.........................] - ETA: 0s - loss: 0.0346 - MAE: 25708.6074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 44\/117 [==========>...................] - ETA: 0s - loss: 0.0366 - MAE: 26020.6719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 66\/117 [===============>..............] - ETA: 0s - loss: 0.0376 - MAE: 26837.6973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 87\/117 [=====================>........] - ETA: 0s - loss: 0.0408 - MAE: 27510.3613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108\/117 [==========================>...] - ETA: 0s - loss: 0.0413 - MAE: 27569.6523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r117\/117 [==============================] - 0s 2ms\/step - loss: 0.0422 - MAE: 27484.2930\n",
            "\r 1\/10 [==>...........................] - ETA: 1s - loss: 0.0322 - MAE: 25775.3984\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10\/10 [==============================] - 0s 1ms\/step - loss: 0.0578 - MAE: 28782.7910\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "preds = model.predict(test_edited)\n",
        "preds"
      ],
      "execution_count":1,
      "outputs":[
        {
          "ename":"NameError",
          "evalue":"NameError: name 'model' is not defined",
          "traceback":[
            "\u001b[0;31m---------------------------------------------------------------------------",
            "Traceback (most recent call last)",
            "    at line 1 in <module>",
            "NameError: name 'model' is not defined"
          ],
          "output_type":"error"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "output = pd.DataFrame(\n",
        "{\n",
        "    'Id':test_data['Id'],\n",
        "    'SalePrice': np.squeeze(preds)\n",
        "})\n",
        "output"
      ],
      "execution_count":52,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>Id<\/th>\n",
              "      <th>SalePrice<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>1461<\/td>\n",
              "      <td>146976.234375<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>1462<\/td>\n",
              "      <td>86779.765625<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>2<\/th>\n",
              "      <td>1463<\/td>\n",
              "      <td>185110.046875<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>3<\/th>\n",
              "      <td>1464<\/td>\n",
              "      <td>187288.765625<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>4<\/th>\n",
              "      <td>1465<\/td>\n",
              "      <td>164490.046875<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>...<\/th>\n",
              "      <td>...<\/td>\n",
              "      <td>...<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1454<\/th>\n",
              "      <td>2915<\/td>\n",
              "      <td>89231.203125<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1455<\/th>\n",
              "      <td>2916<\/td>\n",
              "      <td>108427.031250<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1456<\/th>\n",
              "      <td>2917<\/td>\n",
              "      <td>187847.781250<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1457<\/th>\n",
              "      <td>2918<\/td>\n",
              "      <td>98239.273438<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1458<\/th>\n",
              "      <td>2919<\/td>\n",
              "      <td>233126.265625<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<p>1459 rows × 2 columns<\/p>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        "jupyter":{
          "source_hidden":false,
          "outputs_hidden":false
        },
        "datalore":{
          "type":"CODE",
          "sheet_delimiter":false
        }
      }
    }
  ],
  "metadata":{
    "datalore":{
      "version":1,
      "computation_mode":"JUPYTER",
      "package_manager":"pip",
      "base_environment":"default",
      "packages":[
        {
          "name":"tensorflow-cpu",
          "version":"2.7.0",
          "source":"PIP"
        }
      ]
    }
  },
  "nbformat":4,
  "nbformat_minor":4
}